{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "project_root = Path.cwd()\n",
        "src_path = project_root / \"src\"\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.append(str(src_path))\n",
        "\n",
        "from musicagent.config import DataConfig, OnlineConfig\n",
        "from musicagent.data import OfflineDataset, collate_fn\n",
        "from musicagent.models import OnlineTransformer\n",
        "\n",
        "# All evaluation utilities from the shared eval module\n",
        "from musicagent.eval import (\n",
        "    chord_length_entropy,\n",
        "    chord_lengths,\n",
        "    decode_tokens,\n",
        "    note_in_chord_at_beat,\n",
        "    note_in_chord_ratio,\n",
        "    onset_interval_emd,\n",
        "    onset_intervals,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/models\n",
        "\n",
        "wandb.login()\n",
        "ARTIFACT_REF = \"marty1ai/musicagent/best-model:v50\"\n",
        "\n",
        "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
        "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "api = wandb.Api()\n",
        "artifact = api.artifact(ARTIFACT_REF, type=\"model\")\n",
        "artifact_dir = Path(artifact.download(root=str(CHECKPOINT_DIR)))\n",
        "\n",
        "pt_files = list(artifact_dir.rglob(\"*.pt\"))\n",
        "CHECKPOINT_PATH = pt_files[0]\n",
        "print(CHECKPOINT_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation config\n",
        "BATCH_SIZE = 32  # Smaller batch since online generation is per-sample\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "SAMPLE = False\n",
        "TEMPERATURE = 1.0\n",
        "\n",
        "d_cfg = DataConfig()\n",
        "m_cfg = OnlineConfig()\n",
        "m_cfg.device = DEVICE\n",
        "device = torch.device(m_cfg.device)\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Test split - use OfflineDataset since we need separate melody/chord for metrics\n",
        "test_ds = OfflineDataset(d_cfg, split=\"test\")\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn,\n",
        ")\n",
        "\n",
        "# Reverse vocab mappings for decoding\n",
        "id_to_melody = {v: k for k, v in test_ds.vocab_melody.items()}\n",
        "id_to_chord = {v: k for k, v in test_ds.vocab_chord.items()}\n",
        "\n",
        "# Model\n",
        "melody_vocab_size = len(test_ds.vocab_melody)\n",
        "chord_vocab_size = len(test_ds.vocab_chord)\n",
        "\n",
        "model = OnlineTransformer(m_cfg, d_cfg, melody_vocab_size, chord_vocab_size).to(device)\n",
        "state = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "\n",
        "print(f\"\\nModel loaded: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "print(f\"Melody vocab: {melody_vocab_size}, Chord vocab: {chord_vocab_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_nic = []\n",
        "all_pred_intervals, all_ref_intervals = [], []\n",
        "all_pred_lengths, all_ref_lengths = [], []\n",
        "\n",
        "num_batches = len(test_loader)\n",
        "print(f\"Evaluating {num_batches} batches...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (src, tgt) in enumerate(test_loader):\n",
        "        src = src.to(device)\n",
        "        batch_size = src.size(0)\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            mel_seq = src[i]\n",
        "            \n",
        "            # Strip SOS/EOS/PAD - online model expects only frame tokens\n",
        "            eos_positions = (mel_seq == d_cfg.eos_id).nonzero(as_tuple=True)[0]\n",
        "            eos_idx = int(eos_positions[0].item()) if len(eos_positions) > 0 else mel_seq.size(0)\n",
        "            melody_frames = mel_seq[1:eos_idx].unsqueeze(0)\n",
        "            \n",
        "            # Generate chord predictions\n",
        "            pred_i = model.generate(\n",
        "                melody_frames,\n",
        "                sos_id=d_cfg.sos_id,\n",
        "                eos_id=d_cfg.eos_id,\n",
        "                temperature=TEMPERATURE,\n",
        "                sample=SAMPLE,\n",
        "            )[0]\n",
        "            \n",
        "            mel_tokens = decode_tokens(mel_seq.cpu().tolist(), id_to_melody)\n",
        "            pred_tokens = decode_tokens(pred_i.cpu().tolist(), id_to_chord)\n",
        "            ref_tokens = decode_tokens(tgt[i].cpu().tolist(), id_to_chord)\n",
        "            \n",
        "            all_nic.append(note_in_chord_ratio(mel_tokens, pred_tokens))\n",
        "            all_pred_intervals.extend(onset_intervals(mel_tokens, pred_tokens))\n",
        "            all_ref_intervals.extend(onset_intervals(mel_tokens, ref_tokens))\n",
        "            all_pred_lengths.extend(chord_lengths(pred_tokens))\n",
        "            all_ref_lengths.extend(chord_lengths(ref_tokens))\n",
        "        \n",
        "        if (batch_idx + 1) % 10 == 0 or batch_idx == num_batches - 1:\n",
        "            print(f\"  Processed {batch_idx + 1}/{num_batches} batches\")\n",
        "\n",
        "# Compute aggregate metrics\n",
        "avg_nic = sum(all_nic) / len(all_nic) if all_nic else 0.0\n",
        "emd = onset_interval_emd(all_pred_intervals, all_ref_intervals)\n",
        "pred_entropy = chord_length_entropy(all_pred_lengths)\n",
        "ref_entropy = chord_length_entropy(all_ref_lengths)\n",
        "\n",
        "print(f\"NiC Ratio:         {avg_nic * 100:.2f}%\")\n",
        "print(f\"Onset Interval EMD:          {emd * 1e3:.2f} × 10⁻³\")\n",
        "print(f\"Chord Length Entropy (pred): {pred_entropy:.2f}\")\n",
        "print(f\"Chord Length Entropy (ref):  {ref_entropy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute per-beat NiC for a subset of samples\n",
        "MAX_BEATS = 32\n",
        "NUM_SAMPLES = min(100, len(test_ds))\n",
        "\n",
        "beat_nic_all = {b: [] for b in range(MAX_BEATS)}\n",
        "\n",
        "print(f\"Analyzing adaptation dynamics on {NUM_SAMPLES} samples...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sample_idx in range(NUM_SAMPLES):\n",
        "        src, tgt = test_ds[sample_idx]\n",
        "        src = src.unsqueeze(0).to(device)\n",
        "        \n",
        "        mel_seq = src[0]\n",
        "        eos_positions = (mel_seq == d_cfg.eos_id).nonzero(as_tuple=True)[0]\n",
        "        eos_idx = int(eos_positions[0].item()) if len(eos_positions) > 0 else mel_seq.size(0)\n",
        "        melody_frames = mel_seq[1:eos_idx].unsqueeze(0)\n",
        "        \n",
        "        pred = model.generate(\n",
        "            melody_frames,\n",
        "            sos_id=d_cfg.sos_id,\n",
        "            eos_id=d_cfg.eos_id,\n",
        "            temperature=TEMPERATURE,\n",
        "            sample=SAMPLE,\n",
        "        )[0]\n",
        "        \n",
        "        mel_tokens = decode_tokens(mel_seq.cpu().tolist(), id_to_melody)\n",
        "        pred_tokens = decode_tokens(pred.cpu().tolist(), id_to_chord)\n",
        "        \n",
        "        # Use the shared note_in_chord_at_beat function\n",
        "        beat_nic = note_in_chord_at_beat(mel_tokens, pred_tokens)\n",
        "        \n",
        "        for beat, nic in beat_nic.items():\n",
        "            if beat < MAX_BEATS and nic is not None:\n",
        "                beat_nic_all[beat].append(nic)\n",
        "        \n",
        "        if (sample_idx + 1) % 25 == 0:\n",
        "            print(f\"  Processed {sample_idx + 1}/{NUM_SAMPLES} samples\")\n",
        "\n",
        "# Compute mean and std per beat\n",
        "beat_means, beat_stds, valid_beats = [], [], []\n",
        "for beat in range(MAX_BEATS):\n",
        "    if len(beat_nic_all[beat]) > 5:\n",
        "        valid_beats.append(beat)\n",
        "        beat_means.append(np.mean(beat_nic_all[beat]))\n",
        "        beat_stds.append(np.std(beat_nic_all[beat]))\n",
        "\n",
        "print(f\"\\nComputed dynamics for beats 0-{max(valid_beats) if valid_beats else 0}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot adaptation dynamics\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "if valid_beats:\n",
        "    plt.plot(valid_beats, beat_means, 'o-', color='#2ecc71', linewidth=2, markersize=4)\n",
        "    plt.fill_between(\n",
        "        valid_beats,\n",
        "        [m - s for m, s in zip(beat_means, beat_stds)],\n",
        "        [m + s for m, s in zip(beat_means, beat_stds)],\n",
        "        alpha=0.3, color='#2ecc71'\n",
        "    )\n",
        "plt.axhline(y=avg_nic, color='#e74c3c', linestyle='--', label=f'Overall NiC ({avg_nic:.2%})')\n",
        "plt.xlabel('Beat', fontsize=12)\n",
        "plt.ylabel('Note-in-Chord Ratio', fontsize=12)\n",
        "plt.title('Adaptation Dynamics: Cold Start', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Histogram of chord lengths\n",
        "plt.subplot(1, 2, 2)\n",
        "max_len = 32\n",
        "pred_hist = [min(l, max_len) for l in all_pred_lengths]\n",
        "ref_hist = [min(l, max_len) for l in all_ref_lengths]\n",
        "\n",
        "plt.hist(ref_hist, bins=range(1, max_len + 2), alpha=0.5, label='Reference', color='#3498db')\n",
        "plt.hist(pred_hist, bins=range(1, max_len + 2), alpha=0.5, label='Predicted', color='#e74c3c')\n",
        "plt.xlabel('Chord Length (frames)', fontsize=12)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.title('Chord Length Distribution', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_example(idx: int, max_tokens: int = 32):\n",
        "    \"\"\"Display a single example generation.\"\"\"\n",
        "    src, tgt = test_ds[idx]\n",
        "    src = src.unsqueeze(0).to(device)\n",
        "    \n",
        "    mel_seq = src[0]\n",
        "    eos_positions = (mel_seq == d_cfg.eos_id).nonzero(as_tuple=True)[0]\n",
        "    eos_idx = int(eos_positions[0].item()) if len(eos_positions) > 0 else mel_seq.size(0)\n",
        "    melody_frames = mel_seq[1:eos_idx].unsqueeze(0)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pred = model.generate(\n",
        "            melody_frames,\n",
        "            sos_id=d_cfg.sos_id,\n",
        "            eos_id=d_cfg.eos_id,\n",
        "            temperature=TEMPERATURE,\n",
        "            sample=SAMPLE,\n",
        "        )[0]\n",
        "    \n",
        "    mel_tokens = decode_tokens(mel_seq[1:eos_idx].cpu().tolist(), id_to_melody)\n",
        "    pred_tokens = decode_tokens(pred.cpu().tolist(), id_to_chord)\n",
        "    ref_tokens = decode_tokens(tgt[1:].cpu().tolist(), id_to_chord)\n",
        "    \n",
        "    nic = note_in_chord_ratio(mel_tokens, pred_tokens)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Example {idx} | NiC: {nic:.2%}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nMelody (first {max_tokens} frames):\")\n",
        "    print(\" \".join(mel_tokens[:max_tokens]))\n",
        "    print(f\"\\nPredicted Chords:\")\n",
        "    print(\" \".join(pred_tokens[:max_tokens]))\n",
        "    print(f\"\\nReference Chords:\")\n",
        "    ref_filtered = [t for t in ref_tokens[:max_tokens] if not t.startswith('<')]\n",
        "    print(\" \".join(ref_filtered))\n",
        "\n",
        "# Show 3 examples\n",
        "for i in [0, 50, 100]:\n",
        "    if i < len(test_ds):\n",
        "        show_example(i)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
