{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0e25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "project_root = Path.cwd()\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "from musicagent.config import DataConfig, ModelConfig\n",
    "from musicagent.dataset import MusicAgentDataset, collate_fn\n",
    "from musicagent.eval.metrics import (\n",
    "    chord_length_entropy,\n",
    "    chord_lengths,\n",
    "    note_in_chord_ratio,\n",
    "    onset_interval_emd,\n",
    "    onset_intervals,\n",
    ")\n",
    "from musicagent.model import OfflineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8aa6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/models\n",
    "\n",
    "wandb.login()\n",
    "ARTIFACT_REF = \"your-entity/your-project/best-model:latest\"\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(ARTIFACT_REF, type=\"model\")\n",
    "artifact_dir = Path(artifact.download(root=str(CHECKPOINT_DIR)))\n",
    "\n",
    "pt_files = list(artifact_dir.rglob(\"*.pt\"))\n",
    "CHECKPOINT_PATH = pt_files[0]\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d8383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval config\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAMPLE = False\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "d_cfg = DataConfig()\n",
    "m_cfg = ModelConfig()\n",
    "m_cfg.device = DEVICE\n",
    "device = torch.device(m_cfg.device)\n",
    "print(device)\n",
    "\n",
    "# Test split\n",
    "test_ds = MusicAgentDataset(d_cfg, split=\"test\")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "id_to_melody = {v: k for k, v in test_ds.vocab_melody.items()}\n",
    "id_to_chord = {v: k for k, v in test_ds.vocab_chord.items()}\n",
    "\n",
    "def decode_tokens(ids, id_to_token):\n",
    "    return [id_to_token.get(int(i), \"<unk>\") for i in ids]\n",
    "\n",
    "# Model\n",
    "vocab_src = len(test_ds.vocab_melody)\n",
    "vocab_tgt = len(test_ds.vocab_chord)\n",
    "\n",
    "model = OfflineTransformer(m_cfg, d_cfg, vocab_src, vocab_tgt).to(device)\n",
    "state = torch.load(CHECKPOINT_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nic = []\n",
    "all_pred_intervals, all_ref_intervals = [], []\n",
    "all_pred_lengths, all_ref_lengths = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt in test_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        pred = model.generate(\n",
    "            src,\n",
    "            max_len=d_cfg.max_len,\n",
    "            sos_id=d_cfg.sos_id,\n",
    "            eos_id=d_cfg.eos_id,\n",
    "            temperature=TEMPERATURE,\n",
    "            sample=SAMPLE,\n",
    "        )\n",
    "\n",
    "        for i in range(src.size(0)):\n",
    "            mel_ids = src[i].cpu().tolist()\n",
    "            pred_ids = pred[i].cpu().tolist()\n",
    "            ref_ids = tgt[i].cpu().tolist()\n",
    "\n",
    "            mel_tokens = decode_tokens(mel_ids, id_to_melody)\n",
    "            pred_tokens = decode_tokens(pred_ids, id_to_chord)\n",
    "            ref_tokens = decode_tokens(ref_ids, id_to_chord)\n",
    "\n",
    "            # NiC ratio\n",
    "            nic = note_in_chord_ratio(mel_tokens, pred_tokens)\n",
    "            all_nic.append(nic)\n",
    "\n",
    "            # Onset intervals\n",
    "            all_pred_intervals.extend(onset_intervals(mel_tokens, pred_tokens))\n",
    "            all_ref_intervals.extend(onset_intervals(mel_tokens, ref_tokens))\n",
    "\n",
    "            # Chord lengths\n",
    "            all_pred_lengths.extend(chord_lengths(pred_tokens))\n",
    "            all_ref_lengths.extend(chord_lengths(ref_tokens))\n",
    "\n",
    "avg_nic = sum(all_nic) / len(all_nic) if all_nic else 0.0\n",
    "emd = onset_interval_emd(all_pred_intervals, all_ref_intervals)\n",
    "pred_entropy = chord_length_entropy(all_pred_lengths)\n",
    "ref_entropy = chord_length_entropy(all_ref_lengths)\n",
    "\n",
    "print(f\"NiC Ratio:         {avg_nic * 100:.2f}%\")\n",
    "print(f\"Onset Interval EMD:          {emd * 1e3:.2f} x 10^-3\")\n",
    "print(f\"Chord Length Entropy (pred): {pred_entropy:.2f}\")\n",
    "print(f\"Chord Length Entropy (ref):  {ref_entropy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
